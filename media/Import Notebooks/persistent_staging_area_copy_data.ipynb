{"cells":[{"cell_type":"markdown","source":["# Parameters Section"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8441ac7f-3a04-4e80-a4fe-42d3152863f3"},{"cell_type":"code","source":["# Parameters\n","source_workspace_name = \"std-000-datamovement\"\n","source_lakehouse_name = \"lh_staging\"\n","source_schema_name = \"dbo\"\n","source_table_name = \"Person_Person\"\n","\n","destination_workspace_name = \"std-000-datamovement\"\n","destination_lakehouse_name = \"lh_operations\"\n","destination_schema_name = \"dbo\"\n","destination_table_name = \"Person\"\n","\n","merge_ID_column =\"BusinessEntityID\"\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"da6dbeeb-4d2d-4c80-beea-73b8caea04c4","normalized_state":"finished","queued_time":"2024-11-24T01:55:25.9435958Z","session_start_time":"2024-11-24T01:55:26.8477284Z","execution_start_time":"2024-11-24T01:55:35.8101128Z","execution_finish_time":"2024-11-24T01:55:38.2154854Z","parent_msg_id":"0bab3353-c2cd-40d3-a552-95b6f362bcfa"},"text/plain":"StatementMeta(, da6dbeeb-4d2d-4c80-beea-73b8caea04c4, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"53da41dc-fe31-4e14-b957-da29d641b73c"},{"cell_type":"code","source":["# Formulate the full table names\n","source_full_table_name = f\"`{source_lakehouse_name}`.`{source_table_name}`\"\n","destination_full_table_name = f\"`{destination_lakehouse_name}`.`{destination_table_name}`\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"da6dbeeb-4d2d-4c80-beea-73b8caea04c4","normalized_state":"finished","queued_time":"2024-11-24T01:55:25.9493141Z","session_start_time":null,"execution_start_time":"2024-11-24T01:55:38.5627639Z","execution_finish_time":"2024-11-24T01:55:38.7955328Z","parent_msg_id":"c124a5ff-aae0-458f-9c71-070f88f2df86"},"text/plain":"StatementMeta(, da6dbeeb-4d2d-4c80-beea-73b8caea04c4, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dc276cbc-bc1e-4d62-bbdf-63806ac5ef07"},{"cell_type":"markdown","source":["# Function to Merge Data Dynamically"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"87739766-41dd-4ba8-b0dd-356e1189d05c"},{"cell_type":"code","source":["def merge_data(source_full_table_name, destination_full_table_name, id_column):\n","    try:\n","        # Get existing columns from the persistent staging area table\n","        existing_columns = [row[0] for row in spark.sql(f\"DESCRIBE {destination_full_table_name}\").collect()]\n","\n","        # Get columns from the source table\n","        source_columns = [row[0] for row in spark.sql(f\"DESCRIBE {source_full_table_name}\").collect()]\n","\n","        # Include only the columns present in both the source and the destination tables\n","        columns_to_use = [col for col in source_columns if col in existing_columns]\n","\n","        # Identifying the columns to be updated dynamically\n","        set_clause = \", \".join([f\"target.{col} = source.{col}\" for col in columns_to_use if col != id_column])\n","        insert_clause = \", \".join([f\"source.{col}\" for col in columns_to_use])\n","        insert_columns = \", \".join(columns_to_use + ['DWIsCurrent', 'DWStartDate', 'DWEndDate'])\n","\n","        merge_query = f\"\"\"\n","        MERGE INTO {destination_full_table_name} AS target\n","        USING {source_full_table_name} AS source\n","        ON target.{id_column} = source.{id_column}\n","        WHEN MATCHED THEN\n","            UPDATE SET {set_clause}, \n","            target.DWIsCurrent = true, \n","            target.DWEndDate = NULL\n","        WHEN NOT MATCHED THEN\n","            INSERT ({insert_columns})\n","            VALUES ({insert_clause}, true, current_timestamp(), NULL)\n","        \"\"\"\n","        spark.sql(merge_query)\n","        print(f\"Data merged into {destination_full_table_name}.\")\n","\n","        description =f\"Table {destination_full_table_name} merge statement finalized.\"\n","        status=\"Succeed\"\n","        return status, description\n","    except Exception as e:\n","        print(f\"An error occurred during the merge operation: {str(e)}\")\n","        description =f\"An error occurred during the merge operation: {str(e)}\"\n","        status=\"Error\"\n","        return status, description"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"da6dbeeb-4d2d-4c80-beea-73b8caea04c4","normalized_state":"finished","queued_time":"2024-11-24T01:55:25.9502356Z","session_start_time":null,"execution_start_time":"2024-11-24T01:55:39.1458262Z","execution_finish_time":"2024-11-24T01:55:39.3748187Z","parent_msg_id":"40049e82-a07f-448d-9f3d-32749c69ec30"},"text/plain":"StatementMeta(, da6dbeeb-4d2d-4c80-beea-73b8caea04c4, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"17ddf98d-d3e6-4adb-93bb-2804cfced317"},{"cell_type":"code","source":["def merge_data_scdtype2(source_full_table_name, destination_full_table_name, id_column):\n","    try:\n","        # Get existing columns from the destination table\n","        existing_columns = [row[0] for row in spark.sql(f\"DESCRIBE {destination_full_table_name}\").collect()]\n","\n","        # Get columns from the source table\n","        source_columns = [row[0] for row in spark.sql(f\"DESCRIBE {source_full_table_name}\").collect()]\n","\n","        # Include only the columns present in both the source and destination tables\n","        columns_to_use = [col for col in source_columns if col in existing_columns]\n","\n","        # Build column comparison conditions\n","        change_condition = \" OR \".join([f\"target.{col} != source.{col}\" for col in columns_to_use if col != id_column])\n","\n","        # Filter source data to only include rows with changes\n","        source_data_with_changes = f\"\"\"\n","        SELECT source.*\n","        FROM {source_full_table_name} AS source\n","        LEFT JOIN {destination_full_table_name} AS target\n","        ON source.{id_column} = target.{id_column} AND target.DWIsCurrent = true\n","        WHERE {change_condition}\n","        \"\"\"\n","        source_with_changes_view = \"source_with_changes\"\n","        spark.sql(f\"CREATE OR REPLACE TEMP VIEW {source_with_changes_view} AS {source_data_with_changes}\")\n","\n","        # Construct insert columns\n","        insert_clause = \", \".join([f\"source.{col}\" for col in columns_to_use])\n","        insert_columns = \", \".join(columns_to_use + ['DWIsCurrent', 'DWStartDate', 'DWEndDate'])\n","\n","        # Merge query without subqueries\n","        merge_query = f\"\"\"\n","        MERGE INTO {destination_full_table_name} AS target\n","        USING {source_with_changes_view} AS source\n","        ON target.{id_column} = source.{id_column} AND target.DWIsCurrent = true\n","        WHEN MATCHED THEN\n","            UPDATE SET \n","                target.DWIsCurrent = false,\n","                target.DWEndDate = current_timestamp()\n","        WHEN NOT MATCHED THEN\n","            INSERT ({insert_columns})\n","            VALUES ({insert_clause}, true, current_timestamp(), NULL)\n","        \"\"\"\n","        spark.sql(merge_query)\n","\n","        print(f\"Data merged into {destination_full_table_name}.\")\n","\n","        # Drop the temporary view\n","        spark.sql(f\"DROP VIEW IF EXISTS {source_with_changes_view}\")\n","\n","        description = f\"Table {destination_full_table_name} merge completed with change detection.\"\n","        status = \"Succeed\"\n","        return status, description\n","    except Exception as e:\n","        print(f\"An error occurred during the merge operation: {str(e)}\")\n","\n","        # Attempt to drop the view in case of an error\n","        try:\n","            spark.sql(f\"DROP VIEW IF EXISTS {source_with_changes_view}\")\n","        except Exception as drop_error:\n","            print(f\"Failed to drop view {source_with_changes_view}: {drop_error}\")\n","\n","        description = f\"An error occurred during the merge operation: {str(e)}\"\n","        status = \"Error\"\n","        return status, description\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"da6dbeeb-4d2d-4c80-beea-73b8caea04c4","normalized_state":"finished","queued_time":"2024-11-24T02:04:08.5050876Z","session_start_time":null,"execution_start_time":"2024-11-24T02:04:08.8995278Z","execution_finish_time":"2024-11-24T02:04:09.1253035Z","parent_msg_id":"917c7a3f-fc98-4a36-98e7-9e1407cd3538"},"text/plain":"StatementMeta(, da6dbeeb-4d2d-4c80-beea-73b8caea04c4, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"11d8ec73-4d93-44bc-9ca0-3bef9ee3b318"},{"cell_type":"markdown","source":["# Main Script Execution"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4f1aa73d-e66c-4c47-b89e-2925e3945a42"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","import json\n","\n","# Merge data into the persistent staging table\n","status, description = merge_data_scdtype2(source_full_table_name, destination_full_table_name, merge_ID_column)\n","\n","# Prepare the result as a JSON string\n","result = {\n","    \"status\": status,\n","    \"description\": description\n","}\n","\n","# Check the merge status and exit with an error if the merge failed\n","if status == \"Error\":\n","    print(f\"Error: {description}\")\n","    raise Exception(result)\n","\n","# If the merge succeeded, exit normally\n","mssparkutils.notebook.exit(result)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"da6dbeeb-4d2d-4c80-beea-73b8caea04c4","normalized_state":"finished","queued_time":"2024-11-24T02:04:11.5861249Z","session_start_time":null,"execution_start_time":"2024-11-24T02:04:11.9524448Z","execution_finish_time":"2024-11-24T02:04:15.6564508Z","parent_msg_id":"14b69d09-707a-4e0e-8d90-4676fda38fc3"},"text/plain":"StatementMeta(, da6dbeeb-4d2d-4c80-beea-73b8caea04c4, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Data merged into `lh_operations`.`Person`.\nExitValue: {'status': 'Succeed', 'description': 'Table `lh_operations`.`Person` merge completed with change detection.'}"]}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5308dcbb-1ff3-458d-b826-c723be8ade68"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"f092232d-fc7e-43ea-ac3d-ee712051ef1b","default_lakehouse_name":"lh_staging","default_lakehouse_workspace_id":"aea26e7d-22bd-42e6-b54b-24f61091934b","known_lakehouses":[{"id":"d34d3a45-26f5-4f5f-ae2d-adc068210780"},{"id":"11b9748e-3936-4171-a9ed-465a27445ac0"},{"id":"f092232d-fc7e-43ea-ac3d-ee712051ef1b"}]}}},"nbformat":4,"nbformat_minor":5}