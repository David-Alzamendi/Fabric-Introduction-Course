{"cells":[{"cell_type":"markdown","source":["# Parameters Section"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8441ac7f-3a04-4e80-a4fe-42d3152863f3"},{"cell_type":"code","source":["# Parameters\n","source_workspace_name = \"std-001-datamovement\"\n","source_lakehouse_name = \"lh_staging\"\n","source_schema_name = \"dbo\"\n","source_table_name = \"Person_Person\"\n","\n","destination_workspace_name = \"std-001-datamovement\"\n","destination_lakehouse_name = \"lh_operations\"\n","destination_schema_name = \"dbo\"\n","destination_table_name = \"Person\"\n","\n","merge_ID_column =\"BusinessEntityID\"\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"53da41dc-fe31-4e14-b957-da29d641b73c"},{"cell_type":"code","source":["# Formulate the full table names\n","source_full_table_name = f\"`{source_lakehouse_name}`.`{source_table_name}`\"\n","destination_full_table_name = f\"`{destination_lakehouse_name}`.`{destination_table_name}`\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dc276cbc-bc1e-4d62-bbdf-63806ac5ef07"},{"cell_type":"markdown","source":["# Function to Merge Data Dynamically"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"87739766-41dd-4ba8-b0dd-356e1189d05c"},{"cell_type":"code","source":["def merge_data_scdtype1(source_full_table_name, destination_full_table_name, id_column):\n","    try:\n","        # Get columns from source and destination\n","        source_columns = [row[0] for row in spark.sql(f\"DESCRIBE {source_full_table_name}\").collect()]\n","        destination_columns = [row[0] for row in spark.sql(f\"DESCRIBE {destination_full_table_name}\").collect()]\n","\n","        # Use only shared columns, exclude SCD2 fields if present\n","        scd2_cols = {'DWIsCurrent', 'DWStartDate', 'DWEndDate'}\n","        columns_to_use = [col for col in source_columns if col in destination_columns and col not in scd2_cols]\n","\n","        # Generate dynamic clauses\n","        set_clause = \", \".join([f\"target.{col} = source.{col}\" for col in columns_to_use if col != id_column])\n","        insert_clause = \", \".join([f\"source.{col}\" for col in columns_to_use])\n","        insert_columns = \", \".join(columns_to_use)\n","\n","        # Construct and run the MERGE query\n","        merge_query = f\"\"\"\n","        MERGE INTO {destination_full_table_name} AS target\n","        USING {source_full_table_name} AS source\n","        ON target.{id_column} = source.{id_column}\n","        WHEN MATCHED THEN\n","            UPDATE SET {set_clause}\n","        WHEN NOT MATCHED THEN\n","            INSERT ({insert_columns})\n","            VALUES ({insert_clause})\n","        \"\"\"\n","        spark.sql(merge_query)\n","        print(f\"‚úÖ SCD Type 1 merge complete for table {destination_full_table_name}.\")\n","\n","        return \"Succeed\", f\"SCD Type 1 merge completed for {destination_full_table_name}.\"\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error during SCD Type 1 merge: {str(e)}\")\n","        return \"Error\", f\"An error occurred during the SCD Type 1 merge: {str(e)}\"\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"17ddf98d-d3e6-4adb-93bb-2804cfced317"},{"cell_type":"code","source":["def merge_data_scdtype2(source_full_table_name, destination_full_table_name, id_column):\n","    try:\n","        # Columns to compare\n","        source_cols = [row[0] for row in spark.sql(f\"DESCRIBE {source_full_table_name}\").collect()]\n","        dest_cols = [row[0] for row in spark.sql(f\"DESCRIBE {destination_full_table_name}\").collect()]\n","        scd_cols = {id_column, 'DWIsCurrent', 'DWStartDate', 'DWEndDate'}\n","        compare_cols = [col for col in source_cols if col in dest_cols and col not in scd_cols]\n","\n","        # Check if destination is empty\n","        dest_count = spark.table(destination_full_table_name).count()\n","        if dest_count == 0:\n","            print(\"üì≠ Initial load: destination is empty.\")\n","            insert_targets = \", \".join(source_cols + [\"DWIsCurrent\", \"DWStartDate\", \"DWEndDate\"])\n","            insert_values = \", \".join(source_cols + [\"true\", \"current_timestamp()\", \"NULL\"])\n","            spark.sql(f\"\"\"\n","                INSERT INTO {destination_full_table_name} ({insert_targets})\n","                SELECT {insert_values}\n","                FROM {source_full_table_name}\n","            \"\"\")\n","            inserted = spark.table(source_full_table_name).count()\n","            return \"Succeed\", f\"{inserted} row(s) inserted (initial load).\"\n","\n","        # Build change detection condition\n","        change_condition = \" OR \".join([\n","            f\"(source.{col} IS DISTINCT FROM target.{col})\"\n","            for col in compare_cols\n","        ])\n","\n","        temp_view = \"scd2_changes\"\n","        spark.sql(f\"\"\"\n","            CREATE OR REPLACE TEMP VIEW {temp_view} AS\n","            SELECT source.*\n","            FROM {source_full_table_name} AS source\n","            LEFT JOIN (\n","                SELECT * FROM {destination_full_table_name} WHERE DWIsCurrent = true\n","            ) AS target\n","            ON source.{id_column} = target.{id_column}\n","            WHERE target.{id_column} IS NULL OR ({change_condition})\n","        \"\"\")\n","\n","        # Count rows to be inserted\n","        changed_count = spark.sql(f\"SELECT COUNT(*) AS c FROM {temp_view}\").collect()[0][\"c\"]\n","\n","        if changed_count == 0:\n","            spark.sql(f\"DROP VIEW IF EXISTS {temp_view}\")\n","            return \"Succeed\", \"0 rows inserted. No changes detected.\"\n","\n","        # Close previous versions\n","        spark.sql(f\"\"\"\n","            MERGE INTO {destination_full_table_name} AS target\n","            USING {temp_view} AS source\n","            ON target.{id_column} = source.{id_column} AND target.DWIsCurrent = true\n","            WHEN MATCHED THEN\n","              UPDATE SET target.DWIsCurrent = false,\n","                         target.DWEndDate = current_timestamp()\n","        \"\"\")\n","\n","        # Insert new versions\n","        insert_targets = \", \".join(source_cols + [\"DWIsCurrent\", \"DWStartDate\", \"DWEndDate\"])\n","        insert_values = \", \".join(source_cols + [\"true\", \"current_timestamp()\", \"NULL\"])\n","        spark.sql(f\"\"\"\n","            INSERT INTO {destination_full_table_name} ({insert_targets})\n","            SELECT {insert_values}\n","            FROM {temp_view}\n","        \"\"\")\n","\n","        spark.sql(f\"DROP VIEW IF EXISTS {temp_view}\")\n","        return \"Succeed\", f\"{changed_count} row(s) inserted and old versions closed.\"\n","\n","    except Exception as e:\n","        try:\n","            spark.sql(\"DROP VIEW IF EXISTS scd2_changes\")\n","        except:\n","            pass\n","        return \"Error\", f\"‚ùå Merge failed: {str(e)}\"\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"11d8ec73-4d93-44bc-9ca0-3bef9ee3b318"},{"cell_type":"markdown","source":["# Main Script Execution"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4f1aa73d-e66c-4c47-b89e-2925e3945a42"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","import json\n","\n","# Merge data into the persistent staging table\n","status, description = merge_data_scdtype2(source_full_table_name, destination_full_table_name, merge_ID_column)\n","\n","# Prepare the result as a JSON string\n","result = {\n","    \"status\": status,\n","    \"description\": description\n","}\n","\n","# Check the merge status and exit with an error if the merge failed\n","if status == \"Error\":\n","    print(f\"Error: {description}\")\n","    raise Exception(result)\n","\n","# If the merge succeeded, exit normally\n","mssparkutils.notebook.exit(result)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5308dcbb-1ff3-458d-b826-c723be8ade68"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"643fcdf3-a5f4-4ff9-99d5-b36af07e118b","default_lakehouse_name":"lh_operations","default_lakehouse_workspace_id":"008516f4-2653-448c-b413-0f12c1a2a3d9","known_lakehouses":[{"id":"d34d3a45-26f5-4f5f-ae2d-adc068210780"},{"id":"11b9748e-3936-4171-a9ed-465a27445ac0"},{"id":"f092232d-fc7e-43ea-ac3d-ee712051ef1b"},{"id":"896483aa-6cc7-41e9-b2a5-3fc8b7cac3a0"},{"id":"643fcdf3-a5f4-4ff9-99d5-b36af07e118b"}]}}},"nbformat":4,"nbformat_minor":5}