{"cells":[{"cell_type":"markdown","source":["# Parameters Section"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8441ac7f-3a04-4e80-a4fe-42d3152863f3"},{"cell_type":"code","source":["# Parameters\n","source_workspace_name = \"std-000-datamovement\"\n","source_lakehouse_name = \"lh_staging\"\n","source_schema_name = \"dbo\"\n","source_table_name = \"Person_Person\"\n","\n","destination_workspace_name = \"std-000-datamovement\"\n","destination_lakehouse_name = \"lh_operations\"\n","destination_schema_name = \"dbo\"\n","destination_table_name = \"Person\"\n","\n","merge_ID_column =\"BusinessEntityID\"\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"a5ed9434-b35e-48b2-a425-5d89c8867427","normalized_state":"finished","queued_time":"2024-11-24T01:50:45.3815953Z","session_start_time":null,"execution_start_time":"2024-11-24T01:50:45.8046691Z","execution_finish_time":"2024-11-24T01:50:46.097482Z","parent_msg_id":"9bbab1f5-c3fb-4479-a45d-694906b3ad63"},"text/plain":"StatementMeta(, a5ed9434-b35e-48b2-a425-5d89c8867427, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"53da41dc-fe31-4e14-b957-da29d641b73c"},{"cell_type":"code","source":["# Formulate the full table names\n","source_full_table_name = f\"`{source_lakehouse_name}`.`{source_table_name}`\"\n","destination_full_table_name = f\"`{destination_lakehouse_name}`.`{destination_table_name}`\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"a5ed9434-b35e-48b2-a425-5d89c8867427","normalized_state":"finished","queued_time":"2024-11-24T01:50:45.5118671Z","session_start_time":null,"execution_start_time":"2024-11-24T01:50:46.4282196Z","execution_finish_time":"2024-11-24T01:50:46.7564641Z","parent_msg_id":"b258f338-7731-4045-8d5c-7a4340ab68b6"},"text/plain":"StatementMeta(, a5ed9434-b35e-48b2-a425-5d89c8867427, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dc276cbc-bc1e-4d62-bbdf-63806ac5ef07"},{"cell_type":"markdown","source":["# Function to Create or Update the Table Schema"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b1ceece6-0d66-4715-b7d5-5e8694d1c0b1"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.utils import AnalysisException\n","from py4j.protocol import Py4JJavaError\n","from datetime import datetime\n","\n","# Function to create or update the table schema\n","def create_or_update_table(source_full_table_name, destination_full_table_name):\n","    try:\n","        table_exists = False\n","        try:\n","            # Check if table exists\n","            spark.sql(f\"DESCRIBE {destination_full_table_name}\")\n","            table_exists = True\n","            print(f\"Table {destination_full_table_name} already exists.\")\n","            description =f\"Table {destination_full_table_name} already exists.\"\n","            status=\"Succeed\"\n","        except (AnalysisException, Py4JJavaError):\n","            pass\n","        \n","        if table_exists:\n","            # Get existing columns from the persistent staging area table\n","            existing_columns = {row[0]: row[1] for row in spark.sql(f\"DESCRIBE {destination_full_table_name}\").collect()}\n","            \n","            # Get columns and their data types from the source table\n","            source_columns = {row[0]: row[1] for row in spark.sql(f\"DESCRIBE {source_full_table_name}\").collect()}\n","            \n","            # Add new columns and change data types if necessary\n","            for col, dtype in source_columns.items():\n","                if col not in existing_columns:\n","                    # Add new column\n","                    alter_table_query = f\"ALTER TABLE {destination_full_table_name} ADD COLUMNS ({col} {dtype})\"\n","                    spark.sql(alter_table_query)\n","                    print(f\"Added new column {col} to {destination_full_table_name}.\")\n","                elif existing_columns[col] != dtype:\n","                    # Change data type\n","                    alter_table_query = f\"ALTER TABLE {destination_full_table_name} CHANGE COLUMN {col} {col} {dtype}\"\n","                    spark.sql(alter_table_query)\n","                    print(f\"Changed data type of column {col} to {dtype} in {destination_full_table_name}.\")\n","                \n","                description =f\"Table {destination_full_table_name} columns modified.\"\n","                status=\"Succeed\"\n","        \n","        else:\n","            # If table does not exist, create it\n","            source_columns = [f\"{row[0]} {row[1]}\" for row in spark.sql(f\"DESCRIBE {source_full_table_name}\").collect()]\n","            schema_str = \", \".join(source_columns)\n","            additional_columns = \"DWIsCurrent BOOLEAN, DWStartDate TIMESTAMP, DWEndDate TIMESTAMP\"\n","            create_table_query = f\"\"\"\n","            CREATE TABLE {destination_full_table_name} (\n","                {schema_str},\n","                {additional_columns}\n","            )\n","            \"\"\"\n","            print(create_table_query)\n","            spark.sql(create_table_query)\n","            print(f\"Table {destination_full_table_name} created with additional columns.\")\n","            description =f\"Table {destination_full_table_name} created with additional columns.\"\n","            status=\"Succeed\"\n","        return status, description\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        description =f\"An error occurred: {e}\"\n","        status=\"Error\"\n","        return status, description"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"a5ed9434-b35e-48b2-a425-5d89c8867427","normalized_state":"finished","queued_time":"2024-11-24T01:50:45.6106934Z","session_start_time":null,"execution_start_time":"2024-11-24T01:50:47.1097947Z","execution_finish_time":"2024-11-24T01:50:47.3420213Z","parent_msg_id":"2ed22cad-6e38-4d4c-ab65-3a0f6548ecec"},"text/plain":"StatementMeta(, a5ed9434-b35e-48b2-a425-5d89c8867427, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a148252a-2d11-4741-b5e0-d18509dd986e"},{"cell_type":"markdown","source":["# Main Script Execution"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4f1aa73d-e66c-4c47-b89e-2925e3945a42"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","import json\n","\n","# Create or update the persistent staging table schema\n","status, description = create_or_update_table(source_full_table_name, destination_full_table_name)\n","\n","# Prepare the result as a JSON string\n","result = {\n","    \"status\": status,\n","    \"description\": description\n","}\n","\n","# Check the merge status and exit with an error if the merge failed\n","if status == \"Error\":\n","    print(f\"Error: {description}\")\n","    raise Exception(result)\n","\n","# If the merge succeeded, exit normally\n","mssparkutils.notebook.exit(result)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"a5ed9434-b35e-48b2-a425-5d89c8867427","normalized_state":"finished","queued_time":"2024-11-24T01:50:45.7612799Z","session_start_time":null,"execution_start_time":"2024-11-24T01:50:47.6837242Z","execution_finish_time":"2024-11-24T01:51:06.6065989Z","parent_msg_id":"70cfc971-d649-4ad2-a066-0c6d268f1d31"},"text/plain":"StatementMeta(, a5ed9434-b35e-48b2-a425-5d89c8867427, 10, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n            CREATE TABLE `lh_operations`.`Person` (\n                BusinessEntityID int, PersonType string, NameStyle boolean, Title string, FirstName string, MiddleName string, LastName string, Suffix string, EmailPromotion int, AdditionalContactInfo string, Demographics string, rowguid string, ModifiedDate timestamp,\n                DWIsCurrent BOOLEAN, DWStartDate TIMESTAMP, DWEndDate TIMESTAMP\n            )\n            \nTable `lh_operations`.`Person` created with additional columns.\nExitValue: {'status': 'Succeed', 'description': 'Table `lh_operations`.`Person` created with additional columns.'}"]}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5308dcbb-1ff3-458d-b826-c723be8ade68"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"11b9748e-3936-4171-a9ed-465a27445ac0"},{"id":"f092232d-fc7e-43ea-ac3d-ee712051ef1b"}],"default_lakehouse":"f092232d-fc7e-43ea-ac3d-ee712051ef1b","default_lakehouse_name":"lh_staging","default_lakehouse_workspace_id":"aea26e7d-22bd-42e6-b54b-24f61091934b"}}},"nbformat":4,"nbformat_minor":5}